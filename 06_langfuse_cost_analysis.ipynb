{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize Langfuse client\n",
        "langfuse = Langfuse(\n",
        "    public_key=\"pk-lf-3ebaa19e-3978-4f96-a366-03f98697c5e4\",\n",
        "    secret_key=\"sk-lf-3e648242-1c6a-414b-91aa-cf46c91190f2\",\n",
        "    host=\"https://langfuse.kavida.ai\"  # or your self-hosted URL\n",
        ")\n",
        "\n",
        "# Date range: 26 January to 30 January (inclusive)\n",
        "start_date = datetime(2025, 1, 26)   # 26 Jan 2025 00:00:00\n",
        "end_date = datetime(2025, 1, 30, 23, 59, 59)   # 30 Jan 2025 end of day\n",
        "\n",
        "# # Fetch traces for the date range (Langfuse SDK v3 uses api.trace.list)\n",
        "# first_traces = langfuse.api.trace.list(\n",
        "#     from_timestamp=start_date,\n",
        "#     to_timestamp=end_date,\n",
        "#     limit=50\n",
        "# )\n",
        "# total_pages = first_traces.meta.total_pages\n",
        "# for page_num in tqdm(range(2, total_pages + 1)):\n",
        "#     traces = langfuse.api.trace.list(\n",
        "#         page=page_num,\n",
        "#         from_timestamp=start_date,\n",
        "#         to_timestamp=end_date,\n",
        "#         limit=50\n",
        "#     )\n",
        "#     first_traces.data.extend(traces.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "PAGE_LIMIT = 50  # items per page (Langfuse API)\n",
        "\n",
        "def fetch_page_with_retry(page_num, start_date, end_date, langfuse, max_retries=3, base_delay=1):\n",
        "    \"\"\"Fetch a page with exponential backoff retry logic (uses Langfuse SDK v3 api.trace.list)\"\"\"\n",
        "    retries = 0\n",
        "    while retries <= max_retries:\n",
        "        try:\n",
        "            return langfuse.api.trace.list(\n",
        "                page=page_num,\n",
        "                limit=PAGE_LIMIT,\n",
        "                from_timestamp=start_date,\n",
        "                to_timestamp=end_date\n",
        "            ).data\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            if retries > max_retries:\n",
        "                raise\n",
        "            \n",
        "            # Exponential backoff with jitter\n",
        "            delay = base_delay * (2 ** (retries - 1)) + random.uniform(0, 0.5)\n",
        "            print(f\"Error fetching page {page_num}, retrying in {delay:.2f}s... ({retries}/{max_retries})\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "# Get first page to determine total pages (Langfuse SDK v3: api.trace.list)\n",
        "first_traces = langfuse.api.trace.list(\n",
        "    from_timestamp=start_date,\n",
        "    to_timestamp=end_date,\n",
        "    limit=PAGE_LIMIT\n",
        ")\n",
        "\n",
        "total_pages = first_traces.meta.total_pages\n",
        "all_traces = first_traces.data.copy()  # Start with the first page data\n",
        "\n",
        "# Use ThreadPoolExecutor to fetch the remaining pages in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # Create future tasks for each page (skip page 1 as we already have it)\n",
        "    future_to_page = {\n",
        "        executor.submit(fetch_page_with_retry, page_num, start_date, end_date, langfuse): page_num\n",
        "        for page_num in range(2, total_pages + 1)\n",
        "    }\n",
        "    \n",
        "    # Process results as they complete\n",
        "    for future in tqdm(concurrent.futures.as_completed(future_to_page), total=len(future_to_page)):\n",
        "        page_num = future_to_page[future]\n",
        "        try:\n",
        "            page_data = future.result()\n",
        "            all_traces.extend(page_data)\n",
        "        except Exception as exc:\n",
        "            print(f\"Page {page_num} failed after all retry attempts: {exc}\")\n",
        "\n",
        "# Now all_traces contains all the data from all pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/51388 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id='3b133c36-4955-42e0-9dbc-1a2c93fec3c8' timestamp=datetime.datetime(2025, 7, 4, 9, 11, 4, 81000, tzinfo=datetime.timezone.utc) name='ActionItemsSummarizer' input={'messages': [{'role': 'system', 'content': '\\nYou are a helpful assistant that summarizes alerts for a user on the Kavida platform. Kavida helps manage purchase orders, procurement, and the overall supply chain.\\n\\nYour primary goal is to provide clear, actionable summary of alerts, Seamlessly stitching information in sentences while making it readable to human by group sentences in paragraphs/bullet points of line items that are affected, prioritizing the most critical ones based on business impact.\\n\\n**Key details to extract and present for each critical alert:**\\n1.  **Reason for the alert:** Identify this from the *agentpo_reference_text* excluding definition & explanation of terms and present it in **bold** \\n2.  **Source of the information:**\\n    *   Examine *agentpo_reference_text*.\\n    *   If a clear document filename (e.g., an .xlsx, .pdf) is present, cite that filename.\\n    *   If no filename is apparent but a document type (e.g., \"daily customs ship report\", \"PO revision\") is mentioned, cite that document type.\\n    *   If neither a specific filename nor document type can be cleanly extracted, cite \"supplier email\" or \"email\" as the source.\\n    *   **Crucially, include the exact text snippet** from the email, orderbook comment (specifically mentioning \"supplier_comment\"), or document that generated the alert.\\n3.  The **line numbers that are updated** due to the current alert. Group it incase of large number of line numbers.\\n4.  The **change that happened** from original value to the updated information.\\n5.  **Date of communication:** State when the information was received.\\n6.  Treat any key information under key `metadata` as existing information of line item which never gets updated and include in the summary.\\n    *   include `ship_method` ONLY if the due date is updated.\\n7.  **Exclude information:** that is not relevant to the update\\n    *   Exclude definition and explanation of terms from *agentpo_reference_text* column\\n\\n**Output Format:**\\nStart the summary *directly* neatly communicating the information in a human speech. Seamlessly stitch information in human readable sentences that are intuitive to read. Group sentences in paragraphs/bullet points of line items that are affected. Do not use generic introductory phrases like \"Here is a summary of your alerts\" OR include \"summary\" in the generated summary OR markdown heading(s) OR similar boilerplate text before the title.\\n\\n[Describe the most critical issue impacting an order including the line numbers. State the changes that happened. Clearly state the **reason** (bolded) using `agentpo_reference_text` column and email text (bolded) that triggered the alert. Additionally add the supplier comment (bolded). Mention the specific source (e.g., \"Source: FILENAME.xlsx\", \"Source: Daily customs ship report\", \"Source: Supplier Email\"). Include the date the communication was received (e.g., \"Received on: MMMM DD, YY\").]\\n\\nConsolidate low-priority alerts into a single, concise line.\\n\\nExplicitly Avoid: Long alphanumeric IDs, message IDs, terminology explanations enclosed in (parentheses) , raw data structures, or internal tool jargon (like references to agentUX), and generic introductory phrases.\\n            '}, {'role': 'user', 'content': '\\nGenerate a summary in an informative and creative way. You are provided with a list of alerts for a user, sorted with the most critical alert at the top, then by creation timestamp.\\n\\n**Alerts:**\\n| priority_level   | copilot_priority_task                                                             | alert_type                | agentpo_reference_text   |   referenced_line_numbers |\\n|:-----------------|:----------------------------------------------------------------------------------|:--------------------------|:-------------------------|--------------------------:|\\n| high_priority    | Confirm the latest changes to <b>SM323585</b> with new milestone by <b>MOUSER</b> | alert_po_changes_detected | []                       |                        10 |\\n\\nProposed Data Changes & Context:\\nLine number: 10\\n  customer_part_number: S123/24-81819\\n  Metadata:\\n      Ship Method: U2\\n        '}]} output={'content': None, 'role': 'assistant', 'tool_calls': None, 'function_call': None} session_id=None release=None version=None user_id=None metadata={'client_id': '67876201ce430c55bced0991', 'user_id': '678764e7ce430c55bced0998', 'module': 'ActionItemsSummarizer'} tags=[] public=False html_path='/project/clz8g147n00071174ghdnxsdx/traces/3b133c36-4955-42e0-9dbc-1a2c93fec3c8' latency=15.81 total_cost=0.0077654 observations=['time-09-10-48-267168_chatcmpl-d7a050cf-ee95-4783-a692-2c5dc01a02fd'] scores=[] projectId='clz8g147n00071174ghdnxsdx' updatedAt='2025-07-04T09:11:09.928Z' externalId=None createdAt='2025-07-04T09:11:05.000Z' environment='default' bookmarked=False\n",
            "ActionItemsSummarizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "# for trace in tqdm(first_traces.data):\n",
        "for trace in tqdm(all_traces):\n",
        "    # for span in trace.spans:\n",
        "    print(trace)\n",
        "    print(trace.name)\n",
        "    break\n",
        "    # if trace.metadata is not None and hasattr(trace, 'total_cost'):\n",
        "    #     module = trace.metadata.get('module', 'unknown')\n",
        "    #     cost = trace.total_cost or 0\n",
        "    #     data.append({\n",
        "    #         'module': module,\n",
        "    #         'cost': cost,\n",
        "    #         'timestamp': trace.timestamp,\n",
        "    #         'user_id': trace.user_id if hasattr(trace, 'user_id') else trace.metadata.get('user_id', 'unknown'),\n",
        "    #     })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Cost Estimation Function for Document and Purchase Order Processing\n",
        "===================================================================\n",
        "\n",
        "This script provides a comprehensive cost estimation function that:\n",
        "1. Processes langfuse traces from the last 7 days\n",
        "2. Calculates costs for Gemini 2.5 Pro and Flash models\n",
        "3. Segregates traces into DAES and PO updater sections\n",
        "4. Accounts for document processing (attachment_id) and PO processing (po_number)\n",
        "5. Handles litellm_completion traces without user_id\n",
        "\"\"\"\n",
        "\n",
        "from langfuse import Langfuse\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "\n",
        "class CostEstimator:\n",
        "    \"\"\"\n",
        "    A comprehensive cost estimator for processing documents and purchase orders\n",
        "    using langfuse traces and various AI models.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, langfuse_client: Langfuse):\n",
        "        \"\"\"\n",
        "        Initialize the cost estimator with langfuse client.\n",
        "        \n",
        "        Args:\n",
        "            langfuse_client: Initialized Langfuse client\n",
        "        \"\"\"\n",
        "        self.langfuse = langfuse_client\n",
        "        \n",
        "        # Model pricing (per 1M tokens)\n",
        "        self.pricing = {\n",
        "            'gemini_2_5_pro': {\n",
        "                'input_cost': 1.25,  # $1.25 per 1M tokens\n",
        "                'output_cost': 2.50  # $2.50 per 1M tokens\n",
        "            },\n",
        "            'gemini_2_5_flash': {\n",
        "                'input_cost': 0.30,  # $0.30 per 1M tokens\n",
        "                'output_cost': 1.20  # $1.20 per 1M tokens (corrected from 10 which seemed too high)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # DAES trace patterns\n",
        "        self.daes_flash_patterns = [\n",
        "            \"DocExtractor\", \n",
        "            \"TabularContentCommentContextExtractor\", \n",
        "            \"GPTExcelColumnMapper\"\n",
        "        ]\n",
        "        \n",
        "        self.daes_pro_patterns = [\n",
        "            \"GPTCVAttachmentClassification\", \n",
        "            \"GPTCVExcelAttachmentClassification\"\n",
        "        ]\n",
        "        \n",
        "        # PO updater patterns\n",
        "        self.po_updater_patterns = [\n",
        "            \"AgenticPOUpdater\", \n",
        "            \"litellm-completion\"\n",
        "        ]\n",
        "    \n",
        "    def fetch_traces_with_retry(self, page_num: int, start_date: datetime, \n",
        "                               end_date: datetime, max_retries: int = 3, \n",
        "                               base_delay: float = 1.0) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Fetch traces with exponential backoff retry logic.\n",
        "        \n",
        "        Args:\n",
        "            page_num: Page number to fetch\n",
        "            start_date: Start date for traces\n",
        "            end_date: End date for traces\n",
        "            max_retries: Maximum number of retry attempts\n",
        "            base_delay: Base delay for exponential backoff\n",
        "            \n",
        "        Returns:\n",
        "            List of traces for the page\n",
        "        \"\"\"\n",
        "        retries = 0\n",
        "        while retries <= max_retries:\n",
        "            try:\n",
        "                return self.langfuse.api.trace.list(\n",
        "                    page=page_num,\n",
        "                    limit=50,\n",
        "                    from_timestamp=start_date,\n",
        "                    to_timestamp=end_date\n",
        "                ).data\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                if retries > max_retries:\n",
        "                    print(f\"Failed to fetch page {page_num} after {max_retries} retries: {e}\")\n",
        "                    return []\n",
        "                \n",
        "                # Exponential backoff with jitter\n",
        "                delay = base_delay * (2 ** (retries - 1)) + random.uniform(0, 0.5)\n",
        "                print(f\"Error fetching page {page_num}, retrying in {delay:.2f}s... ({retries}/{max_retries})\")\n",
        "                time.sleep(delay)\n",
        "        \n",
        "        return []\n",
        "    \n",
        "    def fetch_all_traces(self, days_back: int = 7) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Fetch all traces from the last N days using parallel processing.\n",
        "        \n",
        "        Args:\n",
        "            days_back: Number of days to look back\n",
        "            \n",
        "        Returns:\n",
        "            List of all traces\n",
        "        \"\"\"\n",
        "        # Calculate date range\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=days_back)\n",
        "        \n",
        "        print(f\"Fetching traces from {start_date} to {end_date}\")\n",
        "        \n",
        "        # Get first page to determine total pages (Langfuse SDK v3: api.trace.list)\n",
        "        try:\n",
        "            first_traces = self.langfuse.api.trace.list(\n",
        "                from_timestamp=start_date,\n",
        "                to_timestamp=end_date,\n",
        "                limit=50\n",
        "            )\n",
        "            total_pages = first_traces.meta.total_pages\n",
        "            all_traces = first_traces.data.copy()\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching first page: {e}\")\n",
        "            return []\n",
        "        \n",
        "        if total_pages <= 1:\n",
        "            return all_traces\n",
        "        \n",
        "        print(f\"Total pages to fetch: {total_pages}\")\n",
        "        \n",
        "        # Use ThreadPoolExecutor to fetch remaining pages in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            future_to_page = {\n",
        "                executor.submit(self.fetch_traces_with_retry, page_num, start_date, end_date): page_num\n",
        "                for page_num in range(2, total_pages + 1)\n",
        "            }\n",
        "            \n",
        "            # Process results as they complete\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_page), \n",
        "                             total=len(future_to_page), desc=\"Fetching traces\"):\n",
        "                page_num = future_to_page[future]\n",
        "                try:\n",
        "                    page_data = future.result()\n",
        "                    all_traces.extend(page_data)\n",
        "                except Exception as exc:\n",
        "                    print(f\"Page {page_num} failed: {exc}\")\n",
        "        \n",
        "        print(f\"Total traces fetched: {len(all_traces)}\")\n",
        "        return all_traces\n",
        "    \n",
        "    def extract_tokens_from_trace(self, trace: Any) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Extract input and output tokens from a trace.\n",
        "        \n",
        "        Args:\n",
        "            trace: Langfuse trace object\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (input_tokens, output_tokens)\n",
        "        \"\"\"\n",
        "        input_tokens = 0\n",
        "        output_tokens = 0\n",
        "        \n",
        "        # Try to get tokens from observations (Langfuse SDK v3: api.observations.get)\n",
        "        if hasattr(trace, 'observations'):\n",
        "            for obs_id in trace.observations:\n",
        "                try:\n",
        "                    observation = self.langfuse.api.observations.get(observation_id=obs_id)\n",
        "                    if hasattr(observation, 'usage') and observation.usage:\n",
        "                        usage = observation.usage\n",
        "                        input_tokens += getattr(usage, 'input', 0)\n",
        "                        output_tokens += getattr(usage, 'output', 0)\n",
        "                except Exception as e:\n",
        "                    # Continue processing other observations\n",
        "                    continue\n",
        "        \n",
        "        # Fallback: try to estimate from total_cost if available\n",
        "        if input_tokens == 0 and output_tokens == 0 and hasattr(trace, 'total_cost') and trace.total_cost:\n",
        "            # Use a rough estimation based on average token costs\n",
        "            # This is a fallback estimation\n",
        "            estimated_total_tokens = int(trace.total_cost / 0.002 * 1000000)  # Rough estimation\n",
        "            input_tokens = estimated_total_tokens // 2\n",
        "            output_tokens = estimated_total_tokens // 2\n",
        "        \n",
        "        return input_tokens, output_tokens\n",
        "    \n",
        "    def categorize_trace(self, trace: Any) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        Categorize a trace into DAES or PO updater and determine model type.\n",
        "        \n",
        "        Args:\n",
        "            trace: Langfuse trace object\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (category, model_type) where:\n",
        "            - category: 'daes' or 'po_updater'\n",
        "            - model_type: 'gemini_2_5_pro' or 'gemini_2_5_flash'\n",
        "        \"\"\"\n",
        "        trace_name = trace.name if hasattr(trace, 'name') else ''\n",
        "        \n",
        "        # Check DAES patterns\n",
        "        for pattern in self.daes_flash_patterns:\n",
        "            if trace_name.endswith(pattern):\n",
        "                return 'daes', 'gemini_2_5_flash'\n",
        "        \n",
        "        for pattern in self.daes_pro_patterns:\n",
        "            if trace_name.endswith(pattern):\n",
        "                return 'daes', 'gemini_2_5_pro'\n",
        "        \n",
        "        # Check PO updater patterns\n",
        "        for pattern in self.po_updater_patterns:\n",
        "            if pattern in trace_name:\n",
        "                return 'po_updater', 'gemini_2_5_pro'\n",
        "        \n",
        "        # Default categorization\n",
        "        return 'unknown', 'gemini_2_5_pro'\n",
        "    \n",
        "    def calculate_cost(self, input_tokens: int, output_tokens: int, model_type: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate cost based on tokens and model type.\n",
        "        \n",
        "        Args:\n",
        "            input_tokens: Number of input tokens\n",
        "            output_tokens: Number of output tokens\n",
        "            model_type: Type of model ('gemini_2_5_pro' or 'gemini_2_5_flash')\n",
        "            \n",
        "        Returns:\n",
        "            Total cost in dollars\n",
        "        \"\"\"\n",
        "        if model_type not in self.pricing:\n",
        "            model_type = 'gemini_2_5_pro'  # Default fallback\n",
        "        \n",
        "        pricing = self.pricing[model_type]\n",
        "        \n",
        "        input_cost = (input_tokens / 1_000_000) * pricing['input_cost']\n",
        "        output_cost = (output_tokens / 1_000_000) * pricing['output_cost']\n",
        "        \n",
        "        return input_cost + output_cost\n",
        "    \n",
        "    def process_traces(self, traces: List[Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process all traces and calculate costs.\n",
        "        \n",
        "        Args:\n",
        "            traces: List of langfuse traces\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary containing processed cost data\n",
        "        \"\"\"\n",
        "        processed_data = {\n",
        "            'daes': defaultdict(list),\n",
        "            'po_updater': defaultdict(list),\n",
        "            'unknown': defaultdict(list)\n",
        "        }\n",
        "        \n",
        "        # Track unique documents and POs\n",
        "        unique_attachments = set()\n",
        "        unique_pos = set()\n",
        "        \n",
        "        # Store litellm traces for later distribution\n",
        "        litellm_traces = []\n",
        "        agentic_po_traces = []\n",
        "        \n",
        "        print(\"Processing traces...\")\n",
        "        for trace in tqdm(traces, desc=\"Processing traces\"):\n",
        "            try:\n",
        "                # Extract basic information\n",
        "                user_id = getattr(trace, 'user_id', None)\n",
        "                if not user_id and hasattr(trace, 'metadata') and trace.metadata:\n",
        "                    user_id = trace.metadata.get('user_id', 'unknown')\n",
        "                \n",
        "                # Extract attachment_id and po_number from metadata if available\n",
        "                attachment_id = None\n",
        "                po_number = None\n",
        "                \n",
        "                if hasattr(trace, 'metadata') and trace.metadata:\n",
        "                    attachment_id = trace.metadata.get('attachment_id')\n",
        "                    po_number = trace.metadata.get('po_number')\n",
        "                \n",
        "                # Track unique values\n",
        "                if attachment_id:\n",
        "                    unique_attachments.add(attachment_id)\n",
        "                if po_number:\n",
        "                    unique_pos.add(po_number)\n",
        "                \n",
        "                # Get tokens\n",
        "                input_tokens, output_tokens = self.extract_tokens_from_trace(trace)\n",
        "                \n",
        "                # Categorize trace\n",
        "                category, model_type = self.categorize_trace(trace)\n",
        "                \n",
        "                # Calculate cost\n",
        "                cost = self.calculate_cost(input_tokens, output_tokens, model_type)\n",
        "                \n",
        "                # Store trace data\n",
        "                trace_data = {\n",
        "                    'trace_id': getattr(trace, 'id', 'unknown'),\n",
        "                    'name': getattr(trace, 'name', 'unknown'),\n",
        "                    'user_id': user_id,\n",
        "                    'timestamp': getattr(trace, 'timestamp', None),\n",
        "                    'input_tokens': input_tokens,\n",
        "                    'output_tokens': output_tokens,\n",
        "                    'model_type': model_type,\n",
        "                    'cost': cost,\n",
        "                    'attachment_id': attachment_id,\n",
        "                    'po_number': po_number\n",
        "                }\n",
        "                \n",
        "                # Handle litellm traces separately\n",
        "                if 'litellm-completion' in trace_data['name']:\n",
        "                    litellm_traces.append(trace_data)\n",
        "                elif 'AgenticPOUpdater' in trace_data['name']:\n",
        "                    agentic_po_traces.append(trace_data)\n",
        "                else:\n",
        "                    processed_data[category][user_id or 'unknown'].append(trace_data)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing trace: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Distribute litellm traces equally among AgenticPOUpdater calls (when both exist)\n",
        "        if litellm_traces and agentic_po_traces:\n",
        "            self.distribute_litellm_traces(litellm_traces, agentic_po_traces, processed_data)\n",
        "        elif agentic_po_traces:\n",
        "            # AgenticPOUpdater traces without litellm: add them to processed_data as-is\n",
        "            for trace_data in agentic_po_traces:\n",
        "                processed_data['po_updater'][trace_data['user_id'] or 'unknown'].append(trace_data)\n",
        "        \n",
        "        # Add summary statistics\n",
        "        processed_data['summary'] = {\n",
        "            'total_traces': len(traces),\n",
        "            'unique_attachments': len(unique_attachments),\n",
        "            'unique_pos': len(unique_pos),\n",
        "            'unique_users': len(set(user_id for category_data in processed_data.values() \n",
        "                                   if isinstance(category_data, dict)\n",
        "                                   for user_id in category_data.keys())),\n",
        "            'attachment_list': list(unique_attachments),\n",
        "            'po_list': list(unique_pos)\n",
        "        }\n",
        "        \n",
        "        return processed_data\n",
        "    \n",
        "    def distribute_litellm_traces(self, litellm_traces: List[Dict], \n",
        "                                 agentic_po_traces: List[Dict], \n",
        "                                 processed_data: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Distribute litellm traces equally among AgenticPOUpdater calls.\n",
        "        \n",
        "        Args:\n",
        "            litellm_traces: List of litellm trace data\n",
        "            agentic_po_traces: List of AgenticPOUpdater trace data\n",
        "            processed_data: Main processed data dictionary to update\n",
        "        \"\"\"\n",
        "        if not agentic_po_traces:\n",
        "            # If no AgenticPOUpdater traces, add litellm traces as unknown\n",
        "            for trace_data in litellm_traces:\n",
        "                processed_data['unknown'][trace_data['user_id'] or 'unknown'].append(trace_data)\n",
        "            return\n",
        "        \n",
        "        # Calculate total tokens from litellm traces\n",
        "        total_input_tokens = sum(trace['input_tokens'] for trace in litellm_traces)\n",
        "        total_output_tokens = sum(trace['output_tokens'] for trace in litellm_traces)\n",
        "        \n",
        "        # Distribute equally among AgenticPOUpdater traces\n",
        "        tokens_per_trace = {\n",
        "            'input': total_input_tokens // len(agentic_po_traces),\n",
        "            'output': total_output_tokens // len(agentic_po_traces)\n",
        "        }\n",
        "        \n",
        "        # Add distributed tokens to each AgenticPOUpdater trace\n",
        "        for trace_data in agentic_po_traces:\n",
        "            trace_data['input_tokens'] += tokens_per_trace['input']\n",
        "            trace_data['output_tokens'] += tokens_per_trace['output']\n",
        "            trace_data['cost'] = self.calculate_cost(\n",
        "                trace_data['input_tokens'], \n",
        "                trace_data['output_tokens'], \n",
        "                trace_data['model_type']\n",
        "            )\n",
        "            \n",
        "            # Add to processed data\n",
        "            processed_data['po_updater'][trace_data['user_id'] or 'unknown'].append(trace_data)\n",
        "    \n",
        "    def generate_cost_report(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a comprehensive cost report.\n",
        "        \n",
        "        Args:\n",
        "            processed_data: Processed trace data\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary containing cost report\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            'summary': processed_data['summary'],\n",
        "            'costs_by_category': {},\n",
        "            'costs_by_user': {},\n",
        "            'costs_by_model': defaultdict(float),\n",
        "            'document_processing_costs': {},\n",
        "            'po_processing_costs': {},\n",
        "            'total_cost': 0.0\n",
        "        }\n",
        "        \n",
        "        # Calculate costs by category\n",
        "        for category in ['daes', 'po_updater', 'unknown']:\n",
        "            category_data = processed_data[category]\n",
        "            if not isinstance(category_data, dict):\n",
        "                continue\n",
        "                \n",
        "            category_cost = 0.0\n",
        "            category_tokens = {'input': 0, 'output': 0}\n",
        "            \n",
        "            for user_id, traces in category_data.items():\n",
        "                user_cost = sum(trace['cost'] for trace in traces)\n",
        "                category_cost += user_cost\n",
        "                \n",
        "                # Track tokens\n",
        "                for trace in traces:\n",
        "                    category_tokens['input'] += trace['input_tokens']\n",
        "                    category_tokens['output'] += trace['output_tokens']\n",
        "                    \n",
        "                    # Track model costs\n",
        "                    report['costs_by_model'][trace['model_type']] += trace['cost']\n",
        "            \n",
        "            report['costs_by_category'][category] = {\n",
        "                'total_cost': category_cost,\n",
        "                'total_input_tokens': category_tokens['input'],\n",
        "                'total_output_tokens': category_tokens['output'],\n",
        "                'trace_count': sum(len(traces) for traces in category_data.values())\n",
        "            }\n",
        "            \n",
        "            report['total_cost'] += category_cost\n",
        "        \n",
        "        # Calculate costs by user\n",
        "        for category in ['daes', 'po_updater', 'unknown']:\n",
        "            category_data = processed_data[category]\n",
        "            if not isinstance(category_data, dict):\n",
        "                continue\n",
        "                \n",
        "            for user_id, traces in category_data.items():\n",
        "                if user_id not in report['costs_by_user']:\n",
        "                    report['costs_by_user'][user_id] = {\n",
        "                        'daes': 0.0,\n",
        "                        'po_updater': 0.0,\n",
        "                        'unknown': 0.0,\n",
        "                        'total': 0.0\n",
        "                    }\n",
        "                \n",
        "                user_cost = sum(trace['cost'] for trace in traces)\n",
        "                report['costs_by_user'][user_id][category] = user_cost\n",
        "                report['costs_by_user'][user_id]['total'] += user_cost\n",
        "        \n",
        "        # Calculate per-document and per-PO costs (guard against missing category keys)\n",
        "        if report['summary']['unique_attachments'] > 0 and 'daes' in report['costs_by_category']:\n",
        "            daes_cost = report['costs_by_category']['daes']['total_cost']\n",
        "            report['document_processing_costs'] = {\n",
        "                'total_documents': report['summary']['unique_attachments'],\n",
        "                'total_daes_cost': daes_cost,\n",
        "                'cost_per_document': daes_cost / report['summary']['unique_attachments']\n",
        "            }\n",
        "        \n",
        "        if report['summary']['unique_pos'] > 0 and 'po_updater' in report['costs_by_category']:\n",
        "            po_cost = report['costs_by_category']['po_updater']['total_cost']\n",
        "            report['po_processing_costs'] = {\n",
        "                'total_pos': report['summary']['unique_pos'],\n",
        "                'total_po_cost': po_cost,\n",
        "                'cost_per_po': po_cost / report['summary']['unique_pos']\n",
        "            }\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def estimate_future_costs(self, processed_data: Dict[str, Any], \n",
        "                             future_documents: int = 0, \n",
        "                             future_pos: int = 0) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Estimate future costs based on historical data.\n",
        "        \n",
        "        Args:\n",
        "            processed_data: Historical processed data\n",
        "            future_documents: Number of future documents to process\n",
        "            future_pos: Number of future POs to process\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary containing future cost estimates\n",
        "        \"\"\"\n",
        "        report = self.generate_cost_report(processed_data)\n",
        "        \n",
        "        estimates = {\n",
        "            'historical_analysis': {\n",
        "                'documents_processed': report['summary']['unique_attachments'],\n",
        "                'pos_processed': report['summary']['unique_pos'],\n",
        "                'cost_per_document': report.get('document_processing_costs', {}).get('cost_per_document', 0),\n",
        "                'cost_per_po': report.get('po_processing_costs', {}).get('cost_per_po', 0)\n",
        "            },\n",
        "            'future_estimates': {}\n",
        "        }\n",
        "        \n",
        "        if future_documents > 0 and 'document_processing_costs' in report:\n",
        "            doc_cost = report['document_processing_costs']['cost_per_document'] * future_documents\n",
        "            estimates['future_estimates']['document_processing'] = {\n",
        "                'documents': future_documents,\n",
        "                'estimated_cost': doc_cost\n",
        "            }\n",
        "        \n",
        "        if future_pos > 0 and 'po_processing_costs' in report:\n",
        "            po_cost = report['po_processing_costs']['cost_per_po'] * future_pos\n",
        "            estimates['future_estimates']['po_processing'] = {\n",
        "                'pos': future_pos,\n",
        "                'estimated_cost': po_cost\n",
        "            }\n",
        "        \n",
        "        total_future_cost = sum(\n",
        "            est.get('estimated_cost', 0) \n",
        "            for est in estimates['future_estimates'].values()\n",
        "        )\n",
        "        estimates['future_estimates']['total_estimated_cost'] = total_future_cost\n",
        "        \n",
        "        return estimates\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate the cost estimation functionality.\n",
        "    \"\"\"\n",
        "    # Initialize Langfuse client\n",
        "    langfuse = Langfuse(\n",
        "        public_key=\"pk-lf-3ebaa19e-3978-4f96-a366-03f98697c5e4\",\n",
        "        secret_key=\"sk-lf-3e648242-1c6a-414b-91aa-cf46c91190f2\",\n",
        "        host=\"https://langfuse.kavida.ai\"\n",
        "    )\n",
        "    \n",
        "    # Create cost estimator\n",
        "    estimator = CostEstimator(langfuse)\n",
        "    \n",
        "    # Fetch traces from last 7 days\n",
        "    traces = estimator.fetch_all_traces(days_back=7)\n",
        "    \n",
        "    if not traces:\n",
        "        print(\"No traces found!\")\n",
        "        return\n",
        "    \n",
        "    # Process traces\n",
        "    processed_data = estimator.process_traces(traces)\n",
        "    \n",
        "    # Generate cost report\n",
        "    report = estimator.generate_cost_report(processed_data)\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COST ESTIMATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"- Total traces processed: {report['summary']['total_traces']}\")\n",
        "    print(f\"- Unique documents: {report['summary']['unique_attachments']}\")\n",
        "    print(f\"- Unique POs: {report['summary']['unique_pos']}\")\n",
        "    print(f\"- Unique users: {report['summary']['unique_users']}\")\n",
        "    print(f\"- Total cost: ${report['total_cost']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nCosts by Category:\")\n",
        "    for category, data in report['costs_by_category'].items():\n",
        "        print(f\"- {category.upper()}: ${data['total_cost']:.4f} \"\n",
        "              f\"({data['trace_count']} traces)\")\n",
        "    \n",
        "    print(f\"\\nCosts by Model:\")\n",
        "    for model, cost in report['costs_by_model'].items():\n",
        "        print(f\"- {model}: ${cost:.4f}\")\n",
        "    \n",
        "    if 'document_processing_costs' in report:\n",
        "        doc_costs = report['document_processing_costs']\n",
        "        print(f\"\\nDocument Processing:\")\n",
        "        print(f\"- Cost per document: ${doc_costs['cost_per_document']:.4f}\")\n",
        "        print(f\"- Total documents: {doc_costs['total_documents']}\")\n",
        "    \n",
        "    if 'po_processing_costs' in report:\n",
        "        po_costs = report['po_processing_costs']\n",
        "        print(f\"\\nPO Processing:\")\n",
        "        print(f\"- Cost per PO: ${po_costs['cost_per_po']:.4f}\")\n",
        "        print(f\"- Total POs: {po_costs['total_pos']}\")\n",
        "    \n",
        "    # Example future cost estimation\n",
        "    future_estimates = estimator.estimate_future_costs(\n",
        "        processed_data, \n",
        "        future_documents=100, \n",
        "        future_pos=50\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nFuture Cost Estimates (100 docs, 50 POs):\")\n",
        "    if 'future_estimates' in future_estimates:\n",
        "        for key, estimate in future_estimates['future_estimates'].items():\n",
        "            if key != 'total_estimated_cost':\n",
        "                print(f\"- {key}: ${estimate['estimated_cost']:.4f}\")\n",
        "        print(f\"- Total estimated cost: ${future_estimates['future_estimates']['total_estimated_cost']:.4f}\")\n",
        "    \n",
        "    # Save detailed report to JSON\n",
        "    with open('cost_report.json', 'w') as f:\n",
        "        # Convert datetime objects to strings for JSON serialization\n",
        "        json_report = json.dumps(report, indent=2, default=str)\n",
        "        f.write(json_report)\n",
        "    \n",
        "    print(f\"\\nDetailed report saved to 'cost_report.json'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.00765"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class GeminiFlash:\n",
        "    input_cost_per_million = 0.3 \n",
        "    output_cost_per_million = 2.5\n",
        "\n",
        "    @classmethod\n",
        "    def input_cost(cls, tokens):\n",
        "        return (tokens / 1_000_000) * cls.input_cost_per_million\n",
        "    \n",
        "    @classmethod\n",
        "    def output_cost(cls, tokens):\n",
        "        return (tokens / 1_000_000) * cls.output_cost_per_million\n",
        "\n",
        "class GeminiPro:\n",
        "    input_cost_per_million = 1.25 \n",
        "    output_cost_per_million = 10\n",
        "\n",
        "    @classmethod\n",
        "    def input_cost(cls, tokens):\n",
        "        return (tokens / 1_000_000) * cls.input_cost_per_million\n",
        "    \n",
        "    @classmethod\n",
        "    def output_cost(cls, tokens):\n",
        "        return (tokens / 1_000_000) * cls.output_cost_per_million\n",
        "\n",
        "# attachment\n",
        "# classification\n",
        "clf_model = GeminiPro\n",
        "cost_per_clf = clf_model.input_cost(2000) + clf_model.output_cost(80)\n",
        "\n",
        "extr_model = GeminiFlash\n",
        "cost_per_extr = extr_model.input_cost(7000) + extr_model.output_cost(900)\n",
        "\n",
        "cost_per_attachment = cost_per_clf + cost_per_extr\n",
        "cost_per_attachment  # 0.01 USD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.46680829999999995"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hist_ext_model = GeminiFlash\n",
        "_total_cost = hist_ext_model.input_cost(419686) + hist_ext_model.output_cost(136361)\n",
        "_total_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.58095"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PO\n",
        "n_pre_update_steps = 4\n",
        "pre_update_model = GeminiPro\n",
        "pre_update_steps_cost = n_pre_update_steps * (pre_update_model.input_cost(9500) + pre_update_model.output_cost(7000))\n",
        "\n",
        "# PO updater\n",
        "n_po_updater_steps = 7\n",
        "po_updater_model = GeminiPro\n",
        "po_updater_steps_cost = n_po_updater_steps * (po_updater_model.input_cost(1900) + po_updater_model.output_cost(2000))\n",
        "\n",
        "# Total cost\n",
        "num_of_emails = 1.2\n",
        "total_cost = (pre_update_steps_cost + po_updater_steps_cost) * num_of_emails\n",
        "total_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.022949999999999998"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(n_attachments_per_day * cost_per_attachment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.8095"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(n_pos_per_day * cost_per_po)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.83245"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost_per_attachment = cost_per_attachment\n",
        "cost_per_po = total_cost\n",
        "\n",
        "n_attachments_per_day = 3\n",
        "n_pos_per_day = 10\n",
        "\n",
        "cost_per_day = (n_attachments_per_day * cost_per_attachment) + (n_pos_per_day * cost_per_po)\n",
        "cost_per_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "174.9735"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost_per_day * 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/139229 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 139229/139229 [00:00<00:00, 427653.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total cost by module for the last 30 days:\n",
            "==========================================\n",
            "GPTOADocExtractor: $688.6593\n",
            "GPTPODocExtractor: $434.7120\n",
            "GPTInvoiceDocExtractor: $267.6328\n",
            "PeriodicPOLatestStateGenerator: $218.3044\n",
            "GPTDeliveryNoteDocExtractor: $71.0199\n",
            "POUpdater: $63.8104\n",
            "GPTCVAttachmentClassificationV2: $59.2851\n",
            "GPTCVAttachmentClassification: $38.4690\n",
            "GPTCVExcelAttachmentClassification: $29.8586\n",
            "TabularContentCommentContextExtractor: $17.3436\n",
            "GPTMaterialCertDocExtractor: $12.5434\n",
            "GPTPackingListDocExtractor: $11.7631\n",
            "GPTDocIsLinkedToPO: $11.3566\n",
            "GPTDocIsPurchaseOrder: $9.2655\n",
            "GPTCVAttachmentPOExtractionEnhanced: $6.9221\n",
            "UserOnboardingPOLatestStateGenerator: $6.8183\n",
            "GPTBillOfLadingDocExtractor: $6.5602\n",
            "GPTAirwayBillDocExtractor: $6.1257\n",
            "GPTWayBillDocExtractor: $2.7506\n",
            "GPTExcelColumnMapper: $0.0525\n",
            "\n",
            "Total overall cost: $1963.2530\n",
            "\n",
            "Detailed Analysis:\n",
            "=================\n",
            "Total number of API calls: 129215\n",
            "\n",
            "Calls per module:\n",
            "GPTPODocExtractor: 35933 calls\n",
            "GPTOADocExtractor: 33765 calls\n",
            "GPTInvoiceDocExtractor: 16511 calls\n",
            "GPTCVAttachmentClassificationV2: 9615 calls\n",
            "GPTCVAttachmentClassification: 6589 calls\n",
            "GPTDeliveryNoteDocExtractor: 5779 calls\n",
            "GPTCVExcelAttachmentClassification: 5502 calls\n",
            "PeriodicPOLatestStateGenerator: 2695 calls\n",
            "POUpdater: 2626 calls\n",
            "GPTMaterialCertDocExtractor: 2331 calls\n",
            "GPTDocIsPurchaseOrder: 2013 calls\n",
            "TabularContentCommentContextExtractor: 1580 calls\n",
            "GPTDocIsLinkedToPO: 1481 calls\n",
            "GPTCVAttachmentPOExtractionEnhanced: 745 calls\n",
            "GPTPackingListDocExtractor: 703 calls\n",
            "UserOnboardingPOLatestStateGenerator: 477 calls\n",
            "GPTAirwayBillDocExtractor: 323 calls\n",
            "GPTBillOfLadingDocExtractor: 280 calls\n",
            "GPTWayBillDocExtractor: 250 calls\n",
            "GPTExcelColumnMapper: 17 calls\n",
            "\n",
            "Average cost per call by module:\n",
            "PeriodicPOLatestStateGenerator: $0.0810\n",
            "POUpdater: $0.0243\n",
            "GPTBillOfLadingDocExtractor: $0.0234\n",
            "GPTOADocExtractor: $0.0204\n",
            "GPTAirwayBillDocExtractor: $0.0190\n",
            "GPTPackingListDocExtractor: $0.0167\n",
            "GPTInvoiceDocExtractor: $0.0162\n",
            "UserOnboardingPOLatestStateGenerator: $0.0143\n",
            "GPTDeliveryNoteDocExtractor: $0.0123\n",
            "GPTPODocExtractor: $0.0121\n",
            "GPTWayBillDocExtractor: $0.0110\n",
            "TabularContentCommentContextExtractor: $0.0110\n",
            "GPTCVAttachmentPOExtractionEnhanced: $0.0093\n",
            "GPTDocIsLinkedToPO: $0.0077\n",
            "GPTCVAttachmentClassificationV2: $0.0062\n",
            "GPTCVAttachmentClassification: $0.0058\n",
            "GPTCVExcelAttachmentClassification: $0.0054\n",
            "GPTMaterialCertDocExtractor: $0.0054\n",
            "GPTDocIsPurchaseOrder: $0.0046\n",
            "GPTExcelColumnMapper: $0.0031\n"
          ]
        }
      ],
      "source": [
        "# Create a list to store relevant data\n",
        "data = []\n",
        "# for trace in tqdm(first_traces.data):\n",
        "for trace in tqdm(all_traces):\n",
        "    # for span in trace.spans:\n",
        "    if trace.metadata is not None and hasattr(trace, 'total_cost'):\n",
        "        module = trace.metadata.get('module', 'unknown')\n",
        "        cost = trace.total_cost or 0\n",
        "        data.append({\n",
        "            'module': module,\n",
        "            'cost': cost,\n",
        "            'timestamp': trace.timestamp,\n",
        "            'user_id': trace.user_id if hasattr(trace, 'user_id') else trace.metadata.get('user_id', 'unknown'),\n",
        "        })\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Group by module and calculate total cost\n",
        "cost_by_module = df.groupby('module')['cost'].sum()\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTotal cost by module (for the fetched date range):\")\n",
        "print(\"==========================================\")\n",
        "cost_by_module = cost_by_module.sort_values(ascending=False)\n",
        "for module, cost in cost_by_module.items():\n",
        "    print(f\"{module}: ${cost:.4f}\")\n",
        "\n",
        "print(\"\\nTotal overall cost: ${:.4f}\".format(cost_by_module.sum()))\n",
        "\n",
        "# Optional: Create a more detailed analysis\n",
        "detailed_analysis = {\n",
        "    'total_cost': cost_by_module.sum(),\n",
        "    'cost_by_module': cost_by_module.to_dict(),\n",
        "    'number_of_calls': df.groupby('module').size().sort_values(ascending=False).to_dict(),\n",
        "    'average_cost_per_call': (df.groupby('module')['cost'].mean().sort_values(ascending=False)).to_dict()\n",
        "}\n",
        "\n",
        "print(\"\\nDetailed Analysis:\")\n",
        "print(\"=================\")\n",
        "print(f\"Total number of API calls: {len(df)}\")\n",
        "print(\"\\nCalls per module:\")\n",
        "for module, calls in detailed_analysis['number_of_calls'].items():\n",
        "    print(f\"{module}: {calls} calls\")\n",
        "\n",
        "print(\"\\nAverage cost per call by module:\")\n",
        "for module, avg_cost in detailed_analysis['average_cost_per_call'].items():\n",
        "    print(f\"{module}: ${avg_cost:.4f}\")\n",
        "\n",
        "# # Optional: Save to CSV\n",
        "# df.to_csv('langfuse_cost_analysis.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "module\n",
              "GPTOADocExtractor                        688.659255\n",
              "GPTPODocExtractor                        434.712050\n",
              "GPTInvoiceDocExtractor                   267.632752\n",
              "PeriodicPOLatestStateGenerator           218.304365\n",
              "GPTDeliveryNoteDocExtractor               71.019870\n",
              "POUpdater                                 63.810372\n",
              "GPTCVAttachmentClassificationV2           59.285058\n",
              "GPTCVAttachmentClassification             38.469003\n",
              "GPTCVExcelAttachmentClassification        29.858602\n",
              "TabularContentCommentContextExtractor     17.343602\n",
              "GPTMaterialCertDocExtractor               12.543408\n",
              "GPTPackingListDocExtractor                11.763053\n",
              "GPTDocIsLinkedToPO                        11.356560\n",
              "GPTDocIsPurchaseOrder                      9.265545\n",
              "GPTCVAttachmentPOExtractionEnhanced        6.922133\n",
              "UserOnboardingPOLatestStateGenerator       6.818268\n",
              "GPTBillOfLadingDocExtractor                6.560225\n",
              "GPTAirwayBillDocExtractor                  6.125667\n",
              "GPTWayBillDocExtractor                     2.750637\n",
              "GPTExcelColumnMapper                       0.052538\n",
              "Name: cost, dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost_by_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Langfuse cost per module per user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e4d61d45682457ca89999d5a75f022c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Select User:', index=28, layout=Layout(width='50%'), options=(('All Users', 'all'), ('62…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Module</th>\n",
              "      <th>Total Cost ($)</th>\n",
              "      <th>Percentage (%)</th>\n",
              "      <th>Number of Calls</th>\n",
              "      <th>Avg Cost/Call ($)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GPTOADocExtractor</td>\n",
              "      <td>$588.1114</td>\n",
              "      <td>67.94%</td>\n",
              "      <td>25535</td>\n",
              "      <td>$0.0230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GPTInvoiceDocExtractor</td>\n",
              "      <td>$195.0551</td>\n",
              "      <td>22.53%</td>\n",
              "      <td>12584</td>\n",
              "      <td>$0.0155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POUpdater</td>\n",
              "      <td>$32.3232</td>\n",
              "      <td>3.73%</td>\n",
              "      <td>1335</td>\n",
              "      <td>$0.0242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GPTPODocExtractor</td>\n",
              "      <td>$17.7234</td>\n",
              "      <td>2.05%</td>\n",
              "      <td>1676</td>\n",
              "      <td>$0.0106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TabularContentCommentContextExtractor</td>\n",
              "      <td>$10.8606</td>\n",
              "      <td>1.25%</td>\n",
              "      <td>590</td>\n",
              "      <td>$0.0184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>GPTCVExcelAttachmentClassification</td>\n",
              "      <td>$7.7939</td>\n",
              "      <td>0.90%</td>\n",
              "      <td>1332</td>\n",
              "      <td>$0.0059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GPTCVAttachmentPOExtractionEnhanced</td>\n",
              "      <td>$3.6294</td>\n",
              "      <td>0.42%</td>\n",
              "      <td>116</td>\n",
              "      <td>$0.0313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GPTCVAttachmentClassification</td>\n",
              "      <td>$2.9925</td>\n",
              "      <td>0.35%</td>\n",
              "      <td>550</td>\n",
              "      <td>$0.0054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GPTPackingListDocExtractor</td>\n",
              "      <td>$2.6413</td>\n",
              "      <td>0.31%</td>\n",
              "      <td>367</td>\n",
              "      <td>$0.0072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GPTDocIsPurchaseOrder</td>\n",
              "      <td>$2.4138</td>\n",
              "      <td>0.28%</td>\n",
              "      <td>637</td>\n",
              "      <td>$0.0038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>GPTDocIsLinkedToPO</td>\n",
              "      <td>$1.3563</td>\n",
              "      <td>0.16%</td>\n",
              "      <td>165</td>\n",
              "      <td>$0.0082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>UserOnboardingPOLatestStateGenerator</td>\n",
              "      <td>$0.7225</td>\n",
              "      <td>0.08%</td>\n",
              "      <td>54</td>\n",
              "      <td>$0.0134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>GPTWayBillDocExtractor</td>\n",
              "      <td>$0.0199</td>\n",
              "      <td>0.00%</td>\n",
              "      <td>2</td>\n",
              "      <td>$0.0099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Module Total Cost ($) Percentage (%)  \\\n",
              "0                       GPTOADocExtractor      $588.1114         67.94%   \n",
              "1                  GPTInvoiceDocExtractor      $195.0551         22.53%   \n",
              "2                               POUpdater       $32.3232          3.73%   \n",
              "3                       GPTPODocExtractor       $17.7234          2.05%   \n",
              "4   TabularContentCommentContextExtractor       $10.8606          1.25%   \n",
              "5      GPTCVExcelAttachmentClassification        $7.7939          0.90%   \n",
              "6     GPTCVAttachmentPOExtractionEnhanced        $3.6294          0.42%   \n",
              "7           GPTCVAttachmentClassification        $2.9925          0.35%   \n",
              "8              GPTPackingListDocExtractor        $2.6413          0.31%   \n",
              "9                   GPTDocIsPurchaseOrder        $2.4138          0.28%   \n",
              "10                     GPTDocIsLinkedToPO        $1.3563          0.16%   \n",
              "11   UserOnboardingPOLatestStateGenerator        $0.7225          0.08%   \n",
              "12                 GPTWayBillDocExtractor        $0.0199          0.00%   \n",
              "\n",
              "    Number of Calls Avg Cost/Call ($)  \n",
              "0             25535           $0.0230  \n",
              "1             12584           $0.0155  \n",
              "2              1335           $0.0242  \n",
              "3              1676           $0.0106  \n",
              "4               590           $0.0184  \n",
              "5              1332           $0.0059  \n",
              "6               116           $0.0313  \n",
              "7               550           $0.0054  \n",
              "8               367           $0.0072  \n",
              "9               637           $0.0038  \n",
              "10              165           $0.0082  \n",
              "11               54           $0.0134  \n",
              "12                2           $0.0099  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create overall summary\n",
        "overall_cost_by_module = df.groupby('module')['cost'].sum().sort_values(ascending=False)\n",
        "total_overall_cost = overall_cost_by_module.sum()\n",
        "\n",
        "print(\"\\nOverall Total cost by module:\")\n",
        "print(\"============================\")\n",
        "for module, cost in overall_cost_by_module.items():\n",
        "    print(f\"{module}: ${cost:.4f}\")\n",
        "print(f\"\\nTotal overall cost: ${total_overall_cost:.4f}\")\n",
        "\n",
        "# Get unique users for dropdown\n",
        "unique_users = sorted(df['user_id'].unique())\n",
        "\n",
        "# Create interactive widget\n",
        "def show_user_analysis(user):\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Display the dropdown again to maintain UI\n",
        "    display(user_dropdown)\n",
        "    \n",
        "    # Filter data for selected user\n",
        "    user_data = df[df['user_id'] == user]\n",
        "    \n",
        "    if user_data.empty:\n",
        "        print(f\"No data available for user: {user}\")\n",
        "        return\n",
        "    \n",
        "    # Calculate costs by module for this user\n",
        "    user_cost_by_module = user_data.groupby('module')['cost'].sum().sort_values(ascending=False)\n",
        "    total_user_cost = user_cost_by_module.sum()\n",
        "    \n",
        "    print(f\"\\nCost Analysis for User: {user}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create and display a formatted table\n",
        "    module_data = []\n",
        "    for module, cost in user_cost_by_module.items():\n",
        "        percent = (cost / total_user_cost) * 100 if total_user_cost > 0 else 0\n",
        "        calls = user_data[user_data['module'] == module].shape[0]\n",
        "        avg_cost = cost / calls if calls > 0 else 0\n",
        "        module_data.append([module, cost, percent, calls, avg_cost])\n",
        "    \n",
        "    # Create DataFrame for better display\n",
        "    result_df = pd.DataFrame(\n",
        "        module_data, \n",
        "        columns=['Module', 'Total Cost ($)', 'Percentage (%)', 'Number of Calls', 'Avg Cost/Call ($)']\n",
        "    )\n",
        "    \n",
        "    # Format the numeric columns\n",
        "    result_df['Total Cost ($)'] = result_df['Total Cost ($)'].map('${:.4f}'.format)\n",
        "    result_df['Percentage (%)'] = result_df['Percentage (%)'].map('{:.2f}%'.format)\n",
        "    result_df['Avg Cost/Call ($)'] = result_df['Avg Cost/Call ($)'].map('${:.4f}'.format)\n",
        "    \n",
        "    display(result_df)\n",
        "    \n",
        "    print(f\"\\nTotal cost for user {user}: ${total_user_cost:.4f}\")\n",
        "    print(f\"Total calls made by user: {user_data.shape[0]}\")\n",
        "    \n",
        "    # Optional: Add visualization\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        user_cost_by_module.plot(kind='bar', color='skyblue')\n",
        "        plt.title(f'Cost by Module for User: {user}')\n",
        "        plt.ylabel('Cost ($)')\n",
        "        plt.xlabel('Module')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not available for visualization.\")\n",
        "\n",
        "# Create the dropdown widget\n",
        "user_dropdown = widgets.Dropdown(\n",
        "    options=[('All Users', 'all')] + [(user, user) for user in unique_users],\n",
        "    description='Select User:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "# Define what happens when dropdown value changes\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        selected_user = change['new']\n",
        "        if selected_user == 'all':\n",
        "            clear_output(wait=True)\n",
        "            display(user_dropdown)\n",
        "            print(\"\\nOverall Total cost by module:\")\n",
        "            print(\"============================\")\n",
        "            for module, cost in overall_cost_by_module.items():\n",
        "                print(f\"{module}: ${cost:.4f}\")\n",
        "            print(f\"\\nTotal overall cost: ${total_overall_cost:.4f}\")\n",
        "        else:\n",
        "            show_user_analysis(selected_user)\n",
        "\n",
        "user_dropdown.observe(on_change)\n",
        "\n",
        "# Display the dropdown\n",
        "display(user_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Errata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Errata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pagination metadata from first fetch (use first_traces, not traces)\n",
        "first_traces.meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Total traces fetched (all pages); first_traces.data has only first page (e.g. 50)\n",
        "len(all_traces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfirst_traces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3410\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmetadata\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Use all_traces (full list); first_traces.data has only PAGE_LIMIT items (e.g. 50)\n",
        "# Ensure index < len(all_traces), e.g. all_traces[0] or all_traces[min(3410, len(all_traces)-1)]\n",
        "all_traces[min(3410, len(all_traces) - 1)].metadata if all_traces else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
